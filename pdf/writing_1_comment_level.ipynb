{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#Data Cleaning: Comment Level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##What this bit-o-code does:\n",
    "Take .json comment (story) data scrapped from /r/WritingPrompts via perl, clean out the absurd data errors, and output a usable .csv for analysis.\n",
    "\n",
    "Note: this file is a Python version (translation?) of the data cleaning process originally implemented in R. The R version of this file can be found here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import os\n",
    "import codecs\n",
    "import os.path\n",
    "import seaborn as sns\n",
    "import textstat as textstat\n",
    "from curses.ascii import isdigit\n",
    "from sklearn import feature_extraction\n",
    "import statsmodels.formula.api as smf  #because ols\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "                                           # because recodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/marvin/Desktop/Insight/WritingPrompts/commentFile_reduce_pd.csv'"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Where is my data?\n",
    "# NOTE: _pd indicates the python version of these files\n",
    "dir_project       = '/home/marvin/Desktop/Insight/WritingPrompts/'\n",
    "dir_in_prompt     = dir_project + 'PromptCSV/'\n",
    "dir_in_story      = dir_project + 'ResponseCSV/'\n",
    "data_prompt_main  = dir_project + 'allPrompt_pd.csv'\n",
    "data_prompt_clean = dir_project + 'allPrompt_edited_pd.csv'\n",
    "data_story_main   = dir_project + 'commentFile_reduce_pd.csv'\n",
    "data_story_clean  = dir_project + 'commentFile_reduce_edited_pd.csv'\n",
    "\n",
    "data_story_main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For github, select one month to work on\n",
    "(Otherwise time / computation constraints kick in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create comment sample\n",
    "allPrompt   = pd.read_csv(data_prompt_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0               int64\n",
       "p_selftext              object\n",
       "p_id                    object\n",
       "p_gilded               float64\n",
       "p_archived              object\n",
       "p_author                object\n",
       "p_score                float64\n",
       "p_over_18               object\n",
       "p_edited                object\n",
       "p_is_self               object\n",
       "p_name                  object\n",
       "p_url                   object\n",
       "p_author_flair_text     object\n",
       "p_title                 object\n",
       "p_created_utc          float64\n",
       "p_ups                  float64\n",
       "p_num_comments         float64\n",
       "p_genre                 object\n",
       "p_editRec                int64\n",
       "p_editTime             float64\n",
       "p_isAmod                 int64\n",
       "p_distinguished        float64\n",
       "p_year                   int64\n",
       "p_month                  int64\n",
       "p_day                    int64\n",
       "p_wDay                   int64\n",
       "p_prompt_length          int64\n",
       "p_all_length             int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allPrompt.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sampleSet = allPrompt[allPrompt['p_year']==2014]\n",
    "sampleSet = sampleSet[sampleSet['p_month']==7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7097, 28)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampleSet.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another Import Loop!\n",
    "Stories-in-prompts were pulled by prompt, excluding any prompt that had no story-level responses. This resulted in approximately 70k files. \n",
    "\n",
    "To combine:\n",
    "loop over these files to create a single large dataset for analysis. Because this is time consuming, save output to a file. If re-running, check if this file exists to save some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# check to make sure that the files exist\n",
    "#  will want to skip comment files that were not pulled\n",
    "#   --- for example: threads that were deleted (null entries)\n",
    "i = 0\n",
    "sampleSet['p_exists'] = 0\n",
    "for i in range(0, sampleSet.shape[0]):\n",
    "    sampleSet['p_exists'].iloc[i] = os.path.isfile(dir_in_story + sampleSet['p_id'].iloc[i] + '.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "p_exists\n",
       "False    1903\n",
       "True     5194\n",
       "dtype: int64"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampleSet['file'] = sampleSet['p_id'] + '.txt'\n",
    "sampleSet.groupby('p_exists').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35085, 26)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 0\n",
    "tempPath    = dir_in_story + sampleSet['p_id'].iloc[i] + '.txt'\n",
    "commentFile = pd.read_csv(tempPath)\n",
    "\n",
    "# Import loop!\n",
    "forceImport = 1 #set to 0 to force a re-import\n",
    "if((forceImport == 0) or not(os.path.isfile(data_story_main)) ):\n",
    "    \n",
    "    for i in range(1, sampleSet.shape[0]): #loop over the sample prompts\n",
    "    \n",
    "        if(sampleSet['p_exists'].iloc[i] == True):\n",
    "            thisPath = dir_in_story + sampleSet['p_id'].iloc[i] + '.txt'\n",
    "            commentFile  = commentFile.append(pd.read_csv(thisPath))\n",
    "                    \n",
    "    commentFile.to_csv(data_story_main)\n",
    "\n",
    "    \n",
    "# Import the combined csv file\n",
    "commentFile = pd.read_csv(data_story_main)\n",
    "commentFile.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Talking Body\n",
    "Clean the stories (stored in 'body'), and then calculate some features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "commentFile['body'] = commentFile['body'].str.replace('\\\\u201c', '')\n",
    "commentFile['body'] = commentFile['body'].str.replace('\\\\u201d', '')\n",
    "commentFile['body'] = commentFile['body'].str.replace('\\\\u2019d', '')\n",
    "commentFile['body'] = commentFile['body'].str.replace('\\\\u2019t', '')\n",
    "commentFile['body'] = commentFile['body'].str.replace('\\\\u2019s', '')\n",
    "commentFile['body'] = commentFile['body'].str.replace('\\\\[', '')\n",
    "commentFile['body'] = commentFile['body'].str.replace('\\\\]', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# syllable counter helper dictionary\n",
    "d = nltk.corpus.cmudict.dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# count sentences\n",
    "commentFile['sentence_count'] = 0\n",
    "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "#       words\n",
    "commentFile['word_count'] = 0\n",
    "#       syllables\n",
    "commentFile['syllable_count'] = 0\n",
    "#       exclamations\n",
    "commentFile['exclaimMark'] = 0\n",
    "#       questions\n",
    "commentFile['questionMark'] = 0\n",
    "#       I-statements\n",
    "commentFile['i_statement'] = 0\n",
    "#       you-statements\n",
    "commentFile['you_statement'] = 0\n",
    "#       And vs and\n",
    "commentFile['uc_and'] = 0\n",
    "commentFile['lc_and'] = 0\n",
    "# paragraph count\n",
    "commentFile['paragraph_count']   = 0\n",
    "\n",
    "for i in range(0, sampleSet.shape[0]):\n",
    "    commentFile['sentence_count'].iloc[i] = len(sent_detector.tokenize(commentFile['body'].iloc[i].strip()))\n",
    "    story_tokens = nltk.tokenize.word_tokenize(commentFile['body'].iloc[i])\n",
    "    commentFile['word_count'].iloc[i]     = len(story_tokens)\n",
    "    \n",
    "    story_syl = 0\n",
    "    for my_word in story_tokens:\n",
    "        thisList = d.get(my_word.lower())\n",
    "        if thisList is None:\n",
    "            thisList = {}\n",
    "\n",
    "        story_syl += len(thisList)\n",
    "\n",
    "    commentFile['syllable_count']  = story_syl\n",
    "    commentFile['exclaimMark']     = commentFile['body'].iloc[i].count('!')\n",
    "    commentFile['questionMark']    = commentFile['body'].iloc[i].count('?')\n",
    "    commentFile['i_statement']     = commentFile['body'].iloc[i].count(' I ')\n",
    "    commentFile['you_statement']   = commentFile['body'].iloc[i].count(' you ') + commentFile['body'].iloc[i].count(' You ')\n",
    "    commentFile['uc_and']          = commentFile['body'].iloc[i].count(' And ')\n",
    "    commentFile['lc_and']          = commentFile['body'].iloc[i].count(' and ')\n",
    "    commentFile['paragraph_count'] = commentFile['body'].iloc[i].count('\\n') \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# convert UC into a percentage\n",
    "tempRec = commentFile['lc_and']\n",
    "tempRec[tempRec == 0] = 1\n",
    "allPrompt['lc_and'] = tempRec\n",
    "commentFile['perStartAnd']    = commentFile['uc_and'] / (commentFile['uc_and'] + commentFile['lc_and'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert questions, etc into percentages (rough)\n",
    "commentFile['perQ'] = commentFile['questionMark'] / commentFile['sentence_count']\n",
    "commentFile['perE'] = commentFile['exclaimMark'] / commentFile['sentence_count']\n",
    "commentFile['perI'] = commentFile['i_statement'] / commentFile['sentence_count']\n",
    "commentFile['perU'] = commentFile['you_statement'] / commentFile['sentence_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# recode toplevel\n",
    "recoder = { 't1' : 0,\n",
    "            't3' : 1}\n",
    "commentFile['topLevel'] = commentFile['parent_id'].str[0:2]\n",
    "commentFile['topLevel'].replace(recoder, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# grade level\n",
    "commentFile['fleschKincaid'] =  0.39 * (commentFile['word_count'] / commentFile['sentence_count']) + 11.80 * (commentFile['syllable_count'] / commentFile['word_count']) - 15.59"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
